[rl]
model = rgl
gamma = 0.9
exploration_fraction = 0
learning_rate = 0.0005
save_freq= 2000
total_timesteps = 200000

[imitation_learning]
il_episodes = 300
il_policy = orca_plus
il_epochs = 100
il_learning_rate = 0.01
# increase the safety space in ORCA demonstration for robot
safety_space = 0.15

[train]
rl_learning_rate = 0.001
# number of batches to train at the end of training episode
train_batches = 100
# training episodes in outer loop
train_episodes = 10000
# number of episodes sampled in one training episode
sample_episodes = 1
target_update_interval = 50
evaluation_interval = 1000
# the memory pool can roughly store 2K episodes, total size = episodes * 50
capacity = 100000
epsilon_start = 0.5
epsilon_end = 0.1
epsilon_decay = 4000
checkpoint_interval = 1000

[trainer]
batch_size = 100

[DQN_SARL]
size = constant

[sarl]
mlp1_dims = 150, 100
mlp2_dims = 100, 50
attention_dims = 100, 100, 1
mlp3_dims = 150, 100, 100, 1
#mlp1_dims = 100, 50
#mlp2_dims = 50, 25
#attention_dims = 50, 25, 1
#mlp3_dims = 100, 50, 25, 1
multiagent_training = false
with_om = false
with_global_state = true
robot_input_size = 9
human_input_size = 5
rewards = success_reward, collision_penalty, timeout, wall_collision_penalty, angular_smoothness_factor, linear_smoothness_factor, progress_factor

[qsarl]
mlp1_dims = 150, 100
mlp2_dims = 100, 50
attention_dims = 100, 100, 1
mlp3_dims = 150, 100, 100
#mlp1_dims = 100, 50
#mlp2_dims = 50, 25
#attention_dims = 50, 25, 1
#mlp3_dims = 100, 50, 25, 1
multiagent_training = false
with_om = false
with_global_state = true
robot_input_size = 9
human_input_size = 5
rewards = success_reward, collision_penalty, timeout, wall_collision_penalty, angular_smoothness_factor, linear_smoothness_factor, progress_factor


[rgl]
multiagent_training = false
num_layer = 2
X_dim = 32
wr_dims = 64, 32
wh_dims = 64, 32
final_state_dim = 32
gcn2_w1_dim = 32
planning_dims = 150, 100, 100, 1
similarity_function = embedded_gaussian
layerwise_graph = true
skip_connection = false
robot_state_dim = 6
human_state_dim = 7
robot_input_size = 9
human_input_size = 5
rewards = success_reward, collision_penalty, timeout, wall_collision_penalty, angular_smoothness_factor, linear_smoothness_factor, progress_factor

[rgl_multistep]
multiagent_training = false
num_layer = 2
X_dim = 32
wr_dims = 64, 32
wh_dims = 64, 32
final_state_dim = 32
gcn2_w1_dim = 32
planning_dims = 150, 100, 100, 1
similarity_function = embedded_gaussian
layerwise_graph = true
skip_connection = false
robot_state_dim = 6
human_state_dim = 7
robot_input_size = 9
human_input_size = 5
rewards = success_reward, collision_penalty, timeout, wall_collision_penalty, angular_smoothness_factor, linear_smoothness_factor, progress_factor